{"cells":[{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":338,"status":"ok","timestamp":1639601526436,"user":{"displayName":"Riesh Bissessur","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-5VxRcsFNrNBH2DZD0G8dY24c24upvrig6zEQtQ=s64","userId":"10703644578294273074"},"user_tz":-120},"id":"OBYwVAWN1N6K"},"outputs":[],"source":["# Imports\n","import curses\n","import time\n","import sys\n","import math\n","import os\n","import random\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import matplotlib.image as img\n","import csv\n","import numpy as np\n","from collections import deque\n","from random import randint\n","from PIL import Image\n","\n","# Tensorflow imports\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential,load_model\n","from tensorflow.keras.layers import Dense, Dropout, Conv2D,Conv1D, MaxPooling2D,MaxPooling1D, Activation, Flatten\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.losses import Huber\n","import tensorflow.python.keras.backend as backend\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1639601528048,"user":{"displayName":"Riesh Bissessur","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-5VxRcsFNrNBH2DZD0G8dY24c24upvrig6zEQtQ=s64","userId":"10703644578294273074"},"user_tz":-120},"id":"EbUebslq1N6M"},"outputs":[],"source":["# RL settings\n","DISCOUNT = 0.89\n","REPLAY_MEMORY_SIZE = 10000  \n","MIN_REPLAY_MEMORY_SIZE = 1000 \n","MINIBATCH_SIZE = 128  \n","UPDATE_TARGET_EVERY = 5  \n","MODEL_NAME = 'snake'\n","LEARNING_RATE = 0.005\n","\n","# Environment settings\n","EPISODES = 2000\n","ENV_SIZE = 10\n","\n","# Exploration settings\n","EPSILON_DECAY = 0.95\n","MIN_EPSILON = 0.05\n","\n","# Stats settings\n","AGGREGATE_STATS_EVERY = 25  \n","SHOW_PREVIEW = False"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1639601529577,"user":{"displayName":"Riesh Bissessur","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-5VxRcsFNrNBH2DZD0G8dY24c24upvrig6zEQtQ=s64","userId":"10703644578294273074"},"user_tz":-120},"id":"CTqqbmkp1N6N"},"outputs":[],"source":["class Field:\n","    #Initialize game field\n","    def __init__(self,size):\n","        self.size = size\n","        # Used for terminal print\n","        self.icons = {\n","            0: ' . ',\n","            1: ' * ',\n","            2: ' # ',\n","            3: ' \u0026 ',\n","            4 : ' 0 '\n","        }\n","        # Used for RGB image\n","        self.d = {\n","            0: (255, 255, 255),\n","            1: (216, 173, 230),\n","            2: (0, 0, 255),\n","            3: (0,255,0),\n","            4: (0,0,0)\n","            }\n","\n","        self.snake_coords = [[5, 3], [5, 4], [5, 5]]\n","        self._generate_field()\n","        self.add_entity()\n","        if self.get_entity_pos()==[-1,-1]:\n","            self.add_entity()\n","\n","    def reset(self):\n","        # Reset field after episode\n","        self._generate_field()\n","        self.add_entity()\n","        self.snake_coords = [[5, 3], [5, 4], [5, 5]]\n","        if self.get_entity_pos()==[-1,-1]:\n","            self.add_entity()\n","        \n","    def add_entity(self):  \n","       # Add food to field \n","        while(True):\n","            i = randint(2, self.size-2)\n","            j = randint(2, self.size-2)\n","            entity = [i, j]\n","            \n","            if entity not in self.snake_coords:\n","                self.field[i][j] = 3\n","                break\n"," \n","    def _generate_field(self):\n","        self.field = np.zeros((self.size, self.size), dtype=np.uint8) \n","    \n","    def _clear_field(self):        \n","        self.field = [[j if j!= 1 and j!= 2 else 0 for j in i] for i in self.field]\n","\n","    def render(self, show):\n","        # Render field enumeration \n","        size = self.size\n","        self._clear_field()\n","\n","        # Render snake on the field\n","        for i, j in self.snake_coords:\n","            self.field[i][j] = 1\n","        if self.get_entity_pos()==[-1,-1]:\n","            self.add_entity()\n","\n","        # Mark head\n","        head = self.snake_coords[len(self.snake_coords)-1]\n","        self.field[head[0]][head[1]] = 2\n","\n","        for i in range(self.size):\n","                self.field[i][0] = 4\n","                self.field[0][i] = 4\n","                self.field[self.size-1][i] = 4\n","                self.field[i][self.size-1] = 4\n","\n","        # Print icons to terminal\n","        if show:\n","            for i in range(self.size):\n","                row = ''\n","                for j in range(self.size):\n","                    row += self.icons[ self.field[i][j] ]\n","                print(row,'\\n')   \n","        return np.array(self.field)\n","\n","\n","    def get_image(self,game_field):\n","        # Convert enumerated field to RGB image\n","        env = np.zeros((self.size, self.size, 3), dtype=np.uint8) \n","        \n","        for i in range(self.size):\n","            for j in range(self.size):\n","                if game_field[i][j]==0:\n","                    env[i][j] = self.d[0]\n","            if game_field[i][j]==1:\n","                    env[i][j] = self.d[1]\n","            if game_field[i][j]==2:\n","                    env[i][j] = self.d[2]\n","            if game_field[i][j]==3:\n","                    env[i][j] = self.d[3]\n","        img = Image.fromarray(env, 'RGB') \n","        return img\n","\n","\n","\n","    def get_entity_pos(self):\n","        for i in range(self.size):\n","            for j in range(self.size):\n","                if self.field[i][j] == 3:\n","                    return [i, j]\n","        return [-1, -1]\n","\n","\n","    def is_snake_eat_entity(self):\n","        entity = self.get_entity_pos()\n","        head = self.snake_coords[-1]\n","        return entity == head\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1639601530022,"user":{"displayName":"Riesh Bissessur","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-5VxRcsFNrNBH2DZD0G8dY24c24upvrig6zEQtQ=s64","userId":"10703644578294273074"},"user_tz":-120},"id":"l6y-nQ-X1N6O"},"outputs":[],"source":["class Snake:\n","    def __init__(self):\n","        self.direction = 0\n","        self.dist =8\n","        # Init coords\n","        self.coords = [[5, 3], [5, 4], [5, 5]]\n","\n","    def reset(self):\n","        self.direction = 0\n","\n","        # Init coords\n","        self.coords = [[5, 3], [5, 4], [5, 5]]\n","        \n","    def set_direction_normal(self, ch):\n","\n","        # Check if wrong direction\n","        if ch == 1 and self.direction == 0:\n","            return\n","        if ch == 0 and self.direction == 1:\n","            return\n","        if ch == 3 and self.direction == 2:\n","            return\n","        if ch == 2 and self.direction == 3:\n","            return \n","\n","        self.direction = ch\n","\n","    def set_direction_random(self):\n","        rand_dir = randint(0, 4)\n","        # Check if wrong direction\n","        if rand_dir == 0 and  self.direction != 0:\n","            self.direction = 1\n","        if rand_dir == 1 and  self.direction != 1:\n","            self.direction = 0\n","        if rand_dir == 2 and self.direction != 2:\n","            self.direction = 2\n","        if rand_dir == 3 and self.direction != 3:\n","            self.direction = 3\n","\n","\n","\n","    def level_up(self):\n","        # get last point direction\n","        a = self.coords[0]\n","        b = self.coords[1]\n","\n","        tail = a[:]\n","\n","        if a[0] \u003c b[0]:\n","            tail[0]-=1\n","        elif a[1] \u003c b[1]:\n","            tail[1]-=1\n","        elif a[0] \u003e b[0]:\n","            tail[0]+=1\n","        elif a[1] \u003e b[1]:\n","            tail[1]+=1\n","\n","        tail = self._check_limit(tail)\n","        self.coords.insert(0, tail)\n","\n","    def is_dead(self):\n","        head = self.coords[-1]\n","        snake_body = self.coords[:-1]\n","\n","        return head in snake_body\n","\n","    def hit_wall(self):\n","        head = self.coords[-1]\n","        self.walls = []\n","        for i in range(self.field.size):\n","            self.walls.append([i,0])\n","            self.walls.append([0,i])\n","            self.walls.append([self.field.size-1,i])\n","            self.walls.append([i,self.field.size-1])\n","\n","        return head in self.walls\n","\n","\n","    def _check_limit(self, point):\n","        # Check field limit\n","        if point[0] \u003e self.field.size-1:\n","            point[0] = 0\n","        elif point[0] \u003c 0:\n","            point[0] = self.field.size-1\n","        elif point[1] \u003c 0:\n","            point[1] = self.field.size-1\n","        elif point[1] \u003e self.field.size-1:\n","            point[1] = 0\n","\n","        return point\n","\n","    def move(self):\n","        # Determine head coords\n","        head = self.coords[-1][:]\n","        food =  self.field.get_entity_pos()\n","        \n","        done=False\n","        # Calc new head coords\n","        if self.direction == 3:\n","            head[0]-=1\n","        elif self.direction == 2:\n","            head[0]+=1\n","        elif self.direction == 0:\n","            head[1]+=1\n","        elif self.direction == 1:\n","            head[1]-=1\n","        dist=math.sqrt((head[0]-food[0])**2+(head[1]-food[1])**2)\n","        if(dist\u003cself.dist):\n","            reward = -0.005\n","            self.dist = dist\n","        else:\n","            reward = -0.01\n","            self.dist = dist\n","\n","        # Check field limit\n","        head = self._check_limit(head)\n","\n","        del(self.coords[0])\n","        self.coords.append(head)\n","        self.field.snake_coords = self.coords\n","\n","        if  self.is_dead():\n","            reward = -5\n","            done = True\n","\n","        if self.hit_wall():\n","            reward = -5\n","            done =True\n","        # check if snake eat an entity\n","        if self.field.is_snake_eat_entity():\n","            #curses.beep()\n","            self.level_up()\n","            self.field.add_entity()\n","            reward = 1\n","        return done,reward\n","\n","\n","    def set_field(self, field):\n","        self.field = field\n","\n","    def get_entity_pos(self):\n","            for i in range(ENV_SIZE):\n","                for j in range(ENV_SIZE):\n","                    if self.field[i][j] == 3:\n","                        return [i, j]\n","\n","            return [-1, -1]"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1639601530374,"user":{"displayName":"Riesh Bissessur","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-5VxRcsFNrNBH2DZD0G8dY24c24upvrig6zEQtQ=s64","userId":"10703644578294273074"},"user_tz":-120},"id":"rTDPIEK81N6O"},"outputs":[],"source":["class Agent:\n","    def __init__(self):\n","\n","        # Main model\n","        self.model = self.create_model()\n","\n","        # Target network\n","        self.target_model = self.create_model()\n","        self.target_model.set_weights(self.model.get_weights())\n","\n","        # An array with last n steps for training\n","        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n","        \n","        self.target_update_counter = 0\n","\n","    def create_model(self):\n","\n","            model = Sequential()\n","\n","            model.add(Conv1D(32,7,strides=2,activation='relu',padding='same', input_shape=(ENV_SIZE, ENV_SIZE)))  \n","            model.add(MaxPooling1D(pool_size=(3),padding='same'))\n","            \n","            model.add(Conv1D(64,3,activation='relu',padding='same'))  \n","            model.add(MaxPooling1D(pool_size=(2),padding='same'))\n","\n","            model.add(Flatten()) \n","            model.add(Dense(128,activation='relu'))\n","            model.add(Dense(128,activation='relu'))\n","            model.add(Dense(4, activation='linear')) \n","            model.compile(loss='log_cosh', optimizer=Adam(learning_rate=LEARNING_RATE), metrics=['accuracy'])\n","            return model\n","\n","\n","    # Adds step's data to a memory(current observation, action, reward, new observation space, done)\n","    def update_replay_memory(self, transition):\n","        self.replay_memory.append(transition)\n","\n","    # Trains main network every step during episode\n","    def train(self, terminal_state, step):\n","\n","        # Start training only if certain number of samples is already saved\n","        if len(self.replay_memory) \u003c MIN_REPLAY_MEMORY_SIZE:\n","            return\n","\n","        # Get a minibatch of random samples \n","        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n","\n","        # Get current states from minibatch, and Q values from NN\n","        current_states = np.array([transition[0] for transition in minibatch])/4\n","        current_qs_list = self.model.predict(current_states)\n","\n","        # Get future states from minibatch and Q values from NN\n","        new_current_states = np.array([transition[3] for transition in minibatch])/4\n","        future_qs_list = self.target_model.predict(new_current_states)\n","    \n","        X = []\n","        y = []\n","\n","        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n","\n","            # If not a terminal state calc new Q-value\n","            if not done:\n","                max_future_q = np.max(future_qs_list[index])\n","                new_q = reward + DISCOUNT * max_future_q\n","            else:\n","                new_q = reward\n","\n","            # Update Q value for given state\n","            current_qs = current_qs_list[index]\n","            current_qs[action] = new_q\n","\n","            # And append to our training data\n","            X.append(current_state)\n","            y.append(current_qs)\n","\n","        # Fit on all samples as one batch, log only on terminal state\n","        self.model.fit(np.array(X)/4, np.array(y), batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False)\n","\n","        # Update target counter\n","        if terminal_state:\n","            self.target_update_counter += 1\n","\n","        # If counter reaches set value, update target network with weights of main network\n","        if self.target_update_counter \u003e UPDATE_TARGET_EVERY:\n","            self.target_model.set_weights(self.model.get_weights())\n","            self.target_update_counter = 0\n","\n","    # Queries main network for Q values given current observation space \n","    def get_qs(self, state):\n","        return self.model.predict(np.array(state).reshape(-1, *state.shape)/4)[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"NoOqV_ks1N6P"},"outputs":[{"name":"stderr","output_type":"stream","text":["\r  0%|          | 0/2000 [00:00\u003c?, ?episodes/s]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-5.09max___-5.09avg___-5.09min__1639601531.model/assets\n"]},{"name":"stderr","output_type":"stream","text":["  1%|1         | 24/2000 [00:07\u003c08:25,  3.91episodes/s]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-4.02max___-4.99avg___-5.09min__1639601538.model/assets\n"]},{"name":"stderr","output_type":"stream","text":["  2%|2         | 49/2000 [00:15\u003c08:30,  3.82episodes/s]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-4.01max___-4.91avg___-5.06min__1639601546.model/assets\n"]},{"name":"stderr","output_type":"stream","text":["  4%|3         | 74/2000 [00:22\u003c08:40,  3.70episodes/s]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-4.01max___-4.83avg___-5.07min__1639601553.model/assets\n"]},{"name":"stderr","output_type":"stream","text":["  5%|4         | 99/2000 [00:29\u003c06:48,  4.66episodes/s]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-4.01max___-4.99avg___-5.06min__1639601560.model/assets\n"]},{"name":"stderr","output_type":"stream","text":["  6%|6         | 124/2000 [00:36\u003c08:50,  3.53episodes/s]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-3.05max___-4.95avg___-5.05min__1639601567.model/assets\n"]},{"name":"stderr","output_type":"stream","text":["  7%|7         | 149/2000 [00:44\u003c07:23,  4.18episodes/s]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-4.02max___-4.95avg___-5.05min__1639601575.model/assets\n"]},{"name":"stderr","output_type":"stream","text":["  9%|8         | 174/2000 [00:52\u003c08:08,  3.74episodes/s]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-5.01max___-5.03avg___-5.07min__1639601583.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 10%|9         | 199/2000 [00:59\u003c07:56,  3.78episodes/s]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-4.03max___-4.95avg___-5.07min__1639601591.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 11%|#1        | 224/2000 [01:19\u003c35:32,  1.20s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-4.01max___-4.99avg___-5.08min__1639601611.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|#2        | 249/2000 [02:05\u003c1:14:28,  2.55s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-3.06max___-4.86avg___-5.12min__1639601659.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 14%|#3        | 274/2000 [02:56\u003c51:11,  1.78s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-4.04max___-4.86avg___-5.12min__1639601709.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 15%|#4        | 299/2000 [03:53\u003c1:05:39,  2.32s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-4.03max___-5.00avg___-5.12min__1639601765.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 16%|#6        | 324/2000 [04:52\u003c1:22:43,  2.96s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-4.08max___-5.00avg___-5.14min__1639601825.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 17%|#7        | 349/2000 [06:10\u003c1:23:45,  3.04s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-3.13max___-4.82avg___-5.15min__1639601903.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 19%|#8        | 374/2000 [07:29\u003c1:12:01,  2.66s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-4.08max___-4.91avg___-5.17min__1639601983.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|#9        | 399/2000 [08:55\u003c1:40:30,  3.77s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-3.08max___-4.63avg___-5.17min__1639602070.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 21%|##1       | 424/2000 [10:28\u003c1:21:41,  3.11s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-4.01max___-4.92avg___-5.28min__1639602164.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 22%|##2       | 449/2000 [11:57\u003c1:36:21,  3.73s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-3.06max___-4.80avg___-5.24min__1639602253.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 24%|##3       | 474/2000 [13:23\u003c1:34:06,  3.70s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-2.16max___-4.43avg___-5.16min__1639602337.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 25%|##4       | 499/2000 [15:00\u003c1:32:01,  3.68s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-3.13max___-4.65avg___-5.23min__1639602435.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 26%|##6       | 524/2000 [16:35\u003c1:29:49,  3.65s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-0.18max___-4.61avg___-5.21min__1639602532.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|##7       | 549/2000 [18:02\u003c1:34:55,  3.92s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-4.08max___-4.92avg___-5.30min__1639602618.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 29%|##8       | 574/2000 [20:09\u003c2:56:24,  7.42s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-3.42max___-4.69avg___-5.42min__1639602744.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 30%|##9       | 599/2000 [21:56\u003c2:08:20,  5.50s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-3.12max___-4.83avg___-5.35min__1639602853.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 31%|###1      | 624/2000 [23:53\u003c2:24:10,  6.29s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-3.10max___-4.80avg___-5.47min__1639602970.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 32%|###2      | 649/2000 [26:35\u003c2:34:12,  6.85s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-1.41max___-4.58avg___-5.76min__1639603133.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 34%|###3      | 674/2000 [29:06\u003c1:56:53,  5.29s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-3.21max___-4.81avg___-5.75min__1639603284.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 35%|###4      | 699/2000 [32:16\u003c1:23:33,  3.85s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-2.54max___-4.72avg___-5.62min__1639603486.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|###6      | 724/2000 [34:31\u003c1:13:24,  3.45s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-3.15max___-4.60avg___-5.28min__1639603609.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 37%|###7      | 749/2000 [37:07\u003c2:19:10,  6.68s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-2.53max___-4.51avg___-5.75min__1639603775.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 39%|###8      | 774/2000 [39:12\u003c1:43:47,  5.08s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-3.14max___-4.67avg___-5.32min__1639603887.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|###9      | 799/2000 [41:29\u003c58:15,  2.91s/episodes]  "]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-1.31max___-4.38avg___-5.35min__1639604024.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 41%|####1     | 824/2000 [44:28\u003c1:34:25,  4.82s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-1.30max___-4.43avg___-5.29min__1639604200.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 42%|####2     | 849/2000 [47:10\u003c2:18:45,  7.23s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-2.28max___-4.42avg___-5.46min__1639604367.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 44%|####3     | 874/2000 [49:43\u003c3:03:12,  9.76s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-2.52max___-4.38avg___-5.62min__1639604531.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 45%|####4     | 899/2000 [52:14\u003c1:18:32,  4.28s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-2.18max___-4.63avg___-5.46min__1639604675.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 46%|####6     | 924/2000 [53:51\u003c1:01:26,  3.43s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-3.08max___-4.36avg___-5.16min__1639604770.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 47%|####7     | 949/2000 [56:11\u003c57:42,  3.29s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake_____0.51max___-4.30avg___-5.76min__1639604911.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 49%|####8     | 974/2000 [58:16\u003c1:17:17,  4.52s/episodes]"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: models/snake____-3.12max___-4.64avg___-5.43min__1639605032.model/assets\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|####9     | 990/2000 [59:45\u003c58:36,  3.48s/episodes]  "]}],"source":["def main():\n","    ep_rewards = []\n","    ep_score = []\n","    avgs = []\n","    max_rewards = []\n","    epsilon = 0.01\n","    random.seed(1)\n","    np.random.seed(1)\n","    tf.random.set_seed(1)\n","\n","    if not os.path.isdir('models'):\n","        os.makedirs('models')\n","    if not os.path.isdir('images'):\n","        os.makedirs('images')\n","        \n","    field = Field(ENV_SIZE)\n","    snake = Snake()\n","    snake.set_field(field)\n","    agent = Agent()\n","\n","    for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n","        \n","\n","        # Restarting episode - reset episode reward and step number\n","        episode_reward = 0\n","        score = 0\n","        step = 1\n","        new_step = 0\n","        average_reward = -11\n","        old_avg = -11\n","\n","        # Reset environment and get initial state\n","        field.reset()\n","        snake.reset()\n","        current_field = field.render(False)\n","        current_state = field.get_image(current_field)\n","\n","        # Reset flag and start iterating until episode ends\n","        done = False\n","        action = 0\n","        while not done:\n","\n","            if np.random.random() \u003e epsilon:\n","                # Get action from Q table\n","                action = np.argmax(agent.get_qs(current_field))\n","                \n","            else:\n","                # Get random action\n","                action = np.random.randint(0, 4)\n","          \n","            snake.set_direction_normal(action)\n","            done, reward= snake.move()\n","            if reward == 1:\n","                score += 1\n","                new_step = step\n","\n","            new_field = field.render(False)\n","            new_state = field.get_image(new_field)\n","\n","            if (step-new_step) \u003e= 100*(round(episode/10000))+100:\n","                    new_step = step\n","                    done = True\n","                    reward = -5\n","\n","            episode_reward += reward\n","\n","            if SHOW_PREVIEW and not episode % AGGREGATE_STATS_EVERY:\n","                img = current_state.resize((400,400))\n","                img.save(f'images/training-ep_{episode}step__{step}.png')\n","\n","            # Every step we update replay memory and train network\n","            agent.update_replay_memory((current_field, action, reward, new_field, done))\n","            agent.train(done, step)\n","            \n","            current_field = new_field\n","            current_state = new_state\n","            step += 1\n","\n","        # Append episode reward to a list \n","        ep_rewards.append(episode_reward)\n","        ep_score.append(score)\n","        if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n","            old_avg = average_reward\n","            average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n","            min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n","            max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n","            avgs.append(average_reward)\n","            max_rewards.append(max_reward)\n","            old_avg = average_reward\n","            #print(f'{episode}Ep_{max_reward:_\u003e7.2f}max_{average_reward:_\u003e7.2f}avg_{min_reward:_\u003e7.2f}min')\n","            \n","            #Save model\n","            agent.model.save(f'models/{MODEL_NAME}__{max_reward:_\u003e7.2f}max_{average_reward:_\u003e7.2f}avg_{min_reward:_\u003e7.2f}min__{int(time.time())}.model')\n","                \n","\n","        # Decay epsilon\n","        if epsilon \u003e MIN_EPSILON:\n","            epsilon *= EPSILON_DECAY\n","            epsilon = max(MIN_EPSILON, epsilon)\n","        \n","        with open(f'Max-rewards.csv', 'w', encoding='UTF8') as f:\n","                        # create the csv writer\n","            writer = csv.writer(f)\n","\n","                        # write a row to the csv file\n","            writer.writerow(max_rewards)\n","\n","        with open('Score.csv', 'w', encoding='UTF8') as f:\n","                        # create the csv writer\n","            writer = csv.writer(f)\n","\n","                        # write a row to the csv file\n","            writer.writerow(ep_score)\n","\n","        with open(f'Average-rewards.csv', 'w', encoding='UTF8') as f:\n","                        # create the csv writer\n","            writer = csv.writer(f)\n","\n","                        # write a row to the csv file\n","            writer.writerow(avgs)\n","\n","    \n","\n","main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PNgROvw41N6P"},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"name":"working.ipynb","version":""},"interpreter":{"hash":"ba40732c5e532bf752e73a8f4652a54f174faf6722ed27948983549882d42ecb"},"kernelspec":{"display_name":"Python 3.9.6 64-bit ('tensorflow': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}